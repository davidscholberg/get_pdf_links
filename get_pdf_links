#!/usr/bin/perl

# Copyright 2013 David Scholberg <recombinant.vector@gmail.com>

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

use WWW::Mechanize;
use HTML::TokeParser;
use strict;

my $usage = <<EOF;
USAGE:

get_pdf_links <search_depth>

 This script recursively searches the urls from pdftribute.net for direct links
 to pdf files. search_depth is a positive integer indicating how deeply you wish
 to follow each url. A search_depth of 1 is recommended and will return plenty
 of pdf links. If you wish to use a higher search depth, you'll probably get
 more pdf links, but the extra ones that you find may be less relevant.

 NOTE: pdf links are sent to stdout and status messages are sent to stderr, so
 it is recommended to redirect stdout to a file like so:

get_pdf_links 1 > pdf_urls.txt
EOF

if ($ARGV[0] eq "") {
    print STDERR $usage;
    exit;
}

# Make stdout unbuffered
select(STDOUT);
$| = 1;

my $pdftribute_url = "http://pdftribute.net/";

our $search_depth = $ARGV[0];

my %urls_to_crawl = ();
my $pdftribute_page_count = 0;
my $url_count = 0;

my $browser = WWW::Mechanize->new(autocheck => 0);
$browser->max_redirect(0);
$browser->get($pdftribute_url);

print STDERR "Gathering twitter URLs from pdftribute.net...\n";

# search every page of pdftribute.net for twitter links
while ($browser->response()->code() != 404) {
    $url_count = scalar(keys %urls_to_crawl);

    # parse out links to sites with pdf articles
    my $parser = HTML::TokeParser->new(\$browser->content());
    while (my $token = $parser->get_tag("div")) {
        # if the class attribute for the div tag is of type "linkalign"
        if ($token->[1]->{class} eq "linkalign") {
            # get the link from the very next anchor tag
            $token = $parser->get_tag("a");
            my $temp_url = $token->[1]->{href};
            # if URL is not overtly mutilated
            if ($temp_url =~ m/^https?:\/\/.+[0-9a-zA-Z\/]$/) {
                # add to urls for crawling
                $urls_to_crawl{$temp_url} = 1;
            }
        }
    }

    # this is required since the site seems to change willy nilly
    if ($url_count == scalar(keys %urls_to_crawl)) {
        last;
    }

    $pdftribute_page_count++;
    $browser->get($pdftribute_url . $pdftribute_page_count);
}

print STDERR "Found a total of " . scalar(keys %urls_to_crawl)
    . " twitter URLs.\n";
print STDERR "Crawling URLs...\n";

our %pdf_urls = ();
our %checked_urls = ();
$checked_urls{$pdftribute_url} = 1;

# set signal handler
$SIG{INT} = \&signal_handler;
$SIG{TERM} = \&signal_handler;

find_pdf_links($search_depth, \%urls_to_crawl);

clean_up();

#
# functions
#

sub clean_up {
    print STDERR "Done. A total of " . scalar(keys %main::pdf_urls)
    . " PDF links were found.\n";
}

sub signal_handler {
# catches INT signal and cleans up
    print STDERR "** Caught signal SIG" . $_[0] . ". Cleaning up...\n";
    clean_up();
    exit;
}

sub merge_hashes {
    # takes two hash references, merges 2nd hash into first
    (my $hash1, my $hash2) = @_;

    while ((my $key, my $value) = each %$hash2) {
        $hash1->{$key} = $value;
    }
}

sub find_pdf_links {
# $search_depth is a non-negative integer indicating how deeply to follow links
# $urls_to_check is a hash reference. The keys are urls that need to be checked
#   for pdfs and the values are dummy values
# no return value
    (my $search_depth, my $urls_to_check) = @_;

    my %urls_to_crawl = (); # hash of urls to check on next deepest level

    # calculate current recursion level
    my $recursion_level = $main::search_depth - $search_depth;

    my $browser = WWW::Mechanize->new(autocheck => 0);
    $browser->agent_alias("Windows Mozilla");

    print STDERR "** Checking URLs on recursion level " . $recursion_level
        . "\n";

    while ((my $url_to_check, my $dummy) = each %$urls_to_check) {
        # if url has already been checked, skip it
        if (exists($main::checked_urls{$url_to_check})) {
            next;
        }

        # if current url is direct pdf link
        if ($url_to_check =~ m/^https?:\/\/.+\.pdf$/) {
            # output url only if it hasn't already been output
            if (!exists($main::pdf_urls{$url_to_check})) {
                $main::pdf_urls{$url_to_check} = 1;
                print $url_to_check . "\n";
            }            
            next; # there is no further to go with the current URL
        }

        # if we're going to follow links more deeply
        if ($search_depth > 0) {
            # do a GET request, since we'll need the page later
            $browser->get($url_to_check);

            # if GET request failed
            if ($browser->response()->is_error()) {
                print STDERR "GET: " . $browser->response()->status_line()
                    . " FOR URL " . $url_to_check . "\n";
                # there is no further to go with the current URL
                next;
            }
        }
        else {
            # only do HEAD request at first, to prevent needless downloading
            $browser->head($url_to_check);

            # if HEAD request failed
            if ($browser->response()->is_error()) {
                # try GET since some servers don't like HEAD requests
                $browser->get($url_to_check);

                # if GET request failed
                if ($browser->response()->is_error()) {
                    print STDERR "GET: " . $browser->response()->status_line()
                        . " FOR URL " . $url_to_check . "\n";
                    # there is no further to go with the current URL
                    next;
                }
            }
        }
        
        # check again if url has already been checked (in case of redirect)
        if (exists($main::checked_urls{$browser->uri()->as_string()})) {
            next;
        }

        # if current url is pdftribute.net
        if ($browser->uri()->as_string() 
            =~ m/https?:\/\/[pP][dD][fF][tT]ribute\.net\/?/) {
            # skip this link to avoid needless circular crawling
            $main::checked_urls{$url_to_check} = 1;
            $main::checked_urls{$browser->uri()->as_string()} = 1;
            next;
        }

        # check again if current url is pdf link (in case of redirect)
        if ($browser->uri()->as_string() =~ m/^https?:\/\/.+\.pdf$/) {
            # output url only if it hasn't been output already
            if (!exists($main::pdf_urls{$browser->uri()->as_string()})) {
                $main::pdf_urls{$browser->uri()->as_string()} = 1;
                print $browser->uri()->as_string() . "\n";
            }
            next; # there is no further to go with the current URL
        }

        # if url content type is not html or status code is redirect
        if (!($browser->content_type() =~ m/text\/html/)
            || $browser->response()->is_redirect()) {
            # skip url
            $main::checked_urls{$url_to_check} = 1;
            $main::checked_urls{$browser->uri()->as_string()} = 1;
            next;
        }

        # if we're following links more deeply
        if ($search_depth > 0) {
            # get all links from a tags on the current page

            my $parser = HTML::TokeParser->new(\$browser->content());
            while (my $token = $parser->get_tag("a")) {
                my $temp_url = $token->[1]->{href};
                if ($temp_url =~ m/^https?:\/\/.+$/) {
                    $urls_to_crawl{$temp_url} = 1;
                }
            }
        }

        $main::checked_urls{$url_to_check} = 1;
        $main::checked_urls{$browser->uri()->as_string()} = 1;
    } # end while

    print STDERR "** Done on recursion level " . $recursion_level . "\n";

    if ($search_depth > 0) {
        # recursively call find_pdf_links
        find_pdf_links($search_depth - 1, \%urls_to_crawl);
    }

    return;
}

