#!/usr/bin/perl

use WWW::Mechanize;
use HTML::TokeParser;
use strict;

my $usage = <<EOF;
USAGE:

get_pdf_links <search_depth>

 This script recursively searches the urls from pdftribute.net for direct links
 to pdf files. search_depth is a positive integer indicating how deeply you wish
 to follow each url. A search_depth of 1 is recommended and will return plenty
 of pdf links. If you wish to use a higher search depth, you'll probably get
 more pdf links, but the extra ones that you find may be less relevant.

 NOTE: pdf links are sent to stdout and status messages are sent to stderr, so
 it is recommended to redirect stdout to a file like so:

get_pdf_links 1 > pdf_urls.txt

 Currently, the list of PDF links is not checked for duplicates. You can easily
 eliminate duplicates using the sort and uniq utilities:

sort pdf_urls.txt | uniq -u
EOF

if ($ARGV[0] eq "") {
    print STDERR $usage;
    exit;
}

# Make stdout unbuffered
select(STDOUT);
$| = 1;

our $search_depth = $ARGV[0];
our $pdf_url_count = 0;

my $parser;
my $token;
my $temp_url;
my $previous_url_count;
my @pdf_site_urls;
# urls are printed to stdout as they're found, so the @pdf_urls variable is not
# needed at the moment, but there may be a use for this variable in the future
my @pdf_urls;

my $pdftribute_page_count = 0;

my $browser = WWW::Mechanize->new();
$browser->max_redirect(0);
$browser->get("http://pdftribute.net/");

print STDERR "Gathering twitter URLs from pdftribute.net...\n";

# search every page of pdftribute.net for twitter links
while (1) {
    $previous_url_count = scalar(@pdf_site_urls);

    # parse out links to sites with pdf articles
    $parser = HTML::TokeParser->new(\$browser->content());
    while ($token = $parser->get_tag("div")) {
        # if the class attribute for the div tag is of type "linkalign"
        if (${${$token}[1]}{class} eq "linkalign") {
            # get the link from the very next anchor tag
            $token = $parser->get_tag("a");
            $temp_url = ${${$token}[1]}{href};
            # if the url is not overtly mutilated, append it to $pdf_site_urls
            # for later crawling
            if ($temp_url =~ m/^https?:\/\/.+[0-9a-zA-Z\/]$/) {
                push(@pdf_site_urls, $temp_url);
            }
        }
    }

    if ($previous_url_count == scalar(@pdf_site_urls)) {
        # no more urls on pdftribute.net
        last;
    }

    $pdftribute_page_count++;
    $browser->get("http://pdftribute.net/" . $pdftribute_page_count);
}

print STDERR "Found a total of " . scalar(@pdf_site_urls) . " twitter URLs.\n";
print STDERR "Crawling URLs...\n";

foreach (@pdf_site_urls) {
    push(@pdf_urls, find_pdf_links($_, $search_depth));
}

print STDERR "Done. A total of " . $pdf_url_count . " PDF links were found.\n";

#
# functions
#

sub find_pdf_links {
    (my $pdf_site_url, my $search_depth) = @_;

    my @pdf_urls;
    my @urls;

    my $browser = WWW::Mechanize->new(autocheck => 0);
    # some of the t.co urls themselves point to other shortened urls    
    $browser->max_redirect(2);
    $browser->agent_alias("Windows Mozilla");
    my $response = $browser->get($pdf_site_url);

    if ($response->is_error()) {
        print STDERR "ERROR: " . $response->status_line() . "\n";
        return; # there is no further to go with the current URL
    }

    # if current url is not pdftribute.net (avoids needless circular crawling)
    if (!($browser->uri()->as_string() =~ m/http:\/\/pdftribute\.net\/?/)) {
        # calculate current recursion level
        my $recursion_level = $main::search_depth - $search_depth + 1;

        # if current url is direct pdf link
        if ($browser->uri()->as_string() =~ m/^https?:\/\/.+\.pdf$/) {
            # update url count
            $main::pdf_url_count++;
            # grab url
            push(@pdf_urls, $browser->uri()->as_string() . "\n");
            print STDERR "[0] Direct URL\n";
            print $browser->uri()->as_string() . "\n";
            print STDERR " ** Total PDF URLs: " . $main::pdf_url_count . "\n";
            return @pdf_urls; # there is no further to go with the current URL
        }

        print STDERR "[" . $recursion_level . "] Getting urls from "
            . $browser->uri()->as_string() . "\n";

        # get all links from a tags on the current page        
        my $pdf_url_count = 0;
        my $temp_url;
        my $token;
        my $parser = HTML::TokeParser->new(\$browser->content());
        while ($token = $parser->get_tag("a")) {
            $temp_url = ${${$token}[1]}{href};
            # if url is to a pdf
            if ($temp_url =~ m/^https?:\/\/.+\.pdf$/) {
                # update url counts
                $main::pdf_url_count++;
                $pdf_url_count++;
                # grab url
                push(@pdf_urls, $temp_url);
                print $temp_url . "\n";
            }
            elsif ($temp_url =~ m/^https?:\/\/.+/) {
                # add to urls for crawling
                push(@urls, $temp_url);
            }
        }

        if ($pdf_url_count > 0) {
            print STDERR " ** PDF URLs found on this site: " . $pdf_url_count
                . "\n";
            print STDERR " ** Total PDF URLs: " . $main::pdf_url_count . "\n";
        }

        if ($search_depth > 1) {
            foreach (@urls) {
                # recursively call find_pdf_links
                push(@pdf_urls, find_pdf_links($_, $search_depth - 1));
            }
        }
    }

    return @pdf_urls;
}





