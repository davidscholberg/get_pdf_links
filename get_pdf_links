#!/usr/bin/perl

# Copyright 2013 David Scholberg <recombinant.vector@gmail.com>

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

use WWW::Mechanize;
use HTML::TokeParser;
use strict;

my $usage = <<EOF;
USAGE:

get_pdf_links <search_depth>

 This script recursively searches the urls from pdftribute.net for direct links
 to pdf files. search_depth is a positive integer indicating how deeply you wish
 to follow each url. A search_depth of 1 is recommended and will return plenty
 of pdf links. If you wish to use a higher search depth, you'll probably get
 more pdf links, but the extra ones that you find may be less relevant.

 NOTE: pdf links are sent to stdout and status messages are sent to stderr, so
 it is recommended to redirect stdout to a file like so:

get_pdf_links 1 > pdf_urls.txt

 Currently, the list of PDF links is not checked for duplicates. You can easily
 eliminate duplicates using the sort and uniq utilities:

sort pdf_urls.txt | uniq
EOF

if ($ARGV[0] eq "") {
    print STDERR $usage;
    exit;
}

# Make stdout unbuffered
select(STDOUT);
$| = 1;

my $pdftribute_url = "http://pdftribute.net/";

our $search_depth = $ARGV[0];
our $urls_checked_count = -1;
our $pdf_url_count = 0;

my %urls_to_crawl;
my $pdftribute_page_count = 0;

my $browser = WWW::Mechanize->new();
$browser->max_redirect(0);
$browser->get($pdftribute_url);

print STDERR "Gathering twitter URLs from pdftribute.net...\n";

# search every page of pdftribute.net for twitter links
while (1) {
    my $previous_url_count = scalar(keys %urls_to_crawl);

    # parse out links to sites with pdf articles
    my $parser = HTML::TokeParser->new(\$browser->content());
    while (my $token = $parser->get_tag("div")) {
        # if the class attribute for the div tag is of type "linkalign"
        if (${${$token}[1]}{class} eq "linkalign") {
            # get the link from the very next anchor tag
            $token = $parser->get_tag("a");
            my $temp_url = ${${$token}[1]}{href};
            # if URL is not overtly mutilated
            if ($temp_url =~ m/^https?:\/\/.+[0-9a-zA-Z\/]$/) {
                # add to urls for crawling
                $urls_to_crawl{$temp_url} = 1;
            }
        }
    }

    if ($previous_url_count == scalar(keys %urls_to_crawl)) {
        # no more urls on pdftribute.net
        last;
    }

    $pdftribute_page_count++;
    $browser->get($pdftribute_url . $pdftribute_page_count);
}

print STDERR "Found a total of " . scalar(keys %urls_to_crawl)
    . " twitter URLs.\n";
print STDERR "Crawling URLs...\n";

my %hashes_of_urls = ($pdftribute_url => \%urls_to_crawl);

my $pdf_urls = find_pdf_links($search_depth, \%hashes_of_urls);

print STDERR "Done. A total of " . $pdf_url_count . " PDF links were found.\n";

#
# functions
#

sub merge_hashes {
    # takes two hash references, merges 2nd hash into first
    (my $hash1, my $hash2) = @_;

    while ((my $key, my $value) = each %$hash2) {
        $hash1->{$key} = $value;
    }
}

sub find_pdf_links {
# $search_depth is a non-negative integer indicating how deeply to follow links
# $hashes_of_urls is a hash reference. The keys are the parent urls and the
#   values are hash references of child urls
# returns hash reference. The keys are the parent urls and the values are hash
# references of child urls pointing to pdfs found on the parent url site
    (my $search_depth, my $hashes_of_urls) = @_;

    my %hashes_of_pdf_urls;
    my %urls_to_crawl;

    # calculate current recursion level
    my $recursion_level = $main::search_depth - $search_depth;

    my $browser = WWW::Mechanize->new(autocheck => 0);
    $browser->agent_alias("Windows Mozilla");

    while ((my $parent_url, my $child_urls) = each %$hashes_of_urls) {
        print STDERR "[" . $recursion_level . "] Getting urls from "
            . $parent_url . "\n";

        $hashes_of_pdf_urls{$parent_url} = {};
        my $pdf_urls = $hashes_of_pdf_urls{$parent_url};

        my $pdf_url_count = 0;

        while ((my $child_url, my $dummy) = each %$child_urls) {
            $main::urls_checked_count++;
            print STDERR "\r" .
                " * URLs checked so far: " . $main::urls_checked_count .
                " | Total PDF URLs: " . $main::pdf_url_count;

            # if current url is direct pdf link
            if ($child_url =~ m/^https?:\/\/.+\.pdf$/) {
                # update url counts
                $main::pdf_url_count++;
                $pdf_url_count++;
                # grab url
                $pdf_urls->{$child_url} = 1;
                print $child_url . "\n";
                next; # there is no further to go with the current URL
            }

            # if we're going to follow links more deeply
            if ($search_depth > 0) {
                # do a GET request, since we'll need the page later
                $browser->get($child_url);

                # if GET request failed
                if ($browser->response()->is_error()) {
                    # there is no further to go with the current URL
                    next;
                }
            }
            else {
                # only do HEAD request at first, to prevent needless downloading
                $browser->head($child_url);

                # if HEAD request failed
                if ($browser->response()->is_error()) {
                    # try GET since some servers don't like HEAD requests
                    $browser->get($child_url);

                    # if GET request failed
                    if ($browser->response()->is_error()) {
                        # there is no further to go with the current URL
                        next;
                    }
                }
            }
            # if current url is pdftribute.net
            if ($browser->uri()->as_string() 
                =~ m/http:\/\/[pP][dD][fF][tT]ribute\.net\/?/) {
                # skip this link to avoid needless circular crawling
                next;
            }

            # check again if current url is pdf link (in case of redirect)
            if ($browser->uri()->as_string() =~ m/^https?:\/\/.+\.pdf$/) {
                # update url counts
                $main::pdf_url_count++;
                $pdf_url_count++;
                # grab url
                $pdf_urls->{$browser->uri()->as_string()} = 1;
                print $browser->uri()->as_string() . "\n";
                next; # there is no further to go with the current URL
            }

            # if url content type is not html or status code is redirect
            if (!($browser->content_type() =~ m/text\/html/)
                || $browser->response()->is_redirect()) {
                # skip url
                next;
            }

            # if we're following links more deeply
            if ($search_depth > 0) {
                # get all links from a tags on the current page

                $urls_to_crawl{$browser->uri()->as_string()} = {};

                my $parser = HTML::TokeParser->new(\$browser->content());
                while (my $token = $parser->get_tag("a")) {
                    my $temp_url = ${${$token}[1]}{href};
                    if ($temp_url =~ m/^https?:\/\/.+$/) {
                        $urls_to_crawl{$browser->uri()->as_string()}->{$temp_url}
                            = 1;
                    }
                }
            }
        } # end while

        # break out of pdf count status line for parent url
        print STDERR "\n";

    } # end while

    if ($search_depth > 0) {
        # recursively call find_pdf_links
        my $temp_pdf_urls = find_pdf_links($search_depth - 1, \%urls_to_crawl);
        merge_hashes(\%hashes_of_pdf_urls, $temp_pdf_urls);
    }

    return \%hashes_of_pdf_urls;
}

